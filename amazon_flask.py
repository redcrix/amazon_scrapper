import numpy as np
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
from multiprocessing import Process, Queue, Pool, Manager
import threading
import sys
from flask import Flask, json, request
app = Flask(__name__)


@app.route("/amazon", methods=['POST', 'GET'])
def scrap():
    time_str = time.strftime("%Y%m%d-%H%M%S")
    if request.method == 'POST':
        if 'product_name' not in request.form:
            return json.dumps({"message": 'No Product Name Found', "code": 404})
        product_name = request.form['product_name']

        if 'price' not in request.form:
            return json.dumps({"message": 'No Price found', "code": 404})
        price = float(request.form['price'])

        if 'price2' not in request.form:
            return json.dumps({"message": 'No Price range found', "code": 404})
        price2 = float(request.form['price2'])

    # use proxies in requests so as to proxy your request via a proxy server
    # as some sites may block the IP if traffic generated by an IP is too high
    proxies = {
        'http': 'http://134.119.205.253:8080',
        'https': 'http://134.119.205.253:8080',
    }
    startTime = time.time()
    qcount = 0
    products = []  # List to store name of the product
    prices = []  # List to store price of the product
    ratings = []  # List to store ratings of the product
    links = []
    no_pages = 7

    def get_data(pageNo, q):
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0",
                   "Accept-Encoding": "gzip, deflate",
                   "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8", "DNT": "1",
                   "Connection": "close", "Upgrade-Insecure-Requests": "1"}

        r = requests.get(url + "&page=" + str(pageNo), headers=headers)  # , proxies=proxies)
        print(r)
        content = r.content
        soup = BeautifulSoup(content)
        # print(soup.encode('utf-8')) # uncomment this in case there is some non UTF-8 character in the content and
        # you get error

        for d in soup.findAll('div', attrs={
            'class': 'sg-col-4-of-12 sg-col-8-of-16 sg-col-16-of-24 sg-col-12-of-20 sg-col-24-of-32 sg-col sg-col-28-of-36 sg-col-20-of-28'}):
            name = d.find('span', attrs={'class': 'a-size-medium a-color-base a-text-normal'})
            price = d.find('span', attrs={'class': 'a-offscreen'})
            rating = d.find('span', attrs={'class': 'a-icon-alt'})
            links = d.find('a', attrs={'class': 'a-link-normal a-text-normal'})
            all = []

            if name is not None:
                all.append(name.text)
            else:
                all.append("unknown-product")

            if price is not None:
                all.append(price.text)
            else:
                all.append('$0')

            if rating is not None:
                all.append(rating.text)
            else:
                all.append('-1')

            if links is not None:
                all.append("https://www.amazon.com" + links.get('href'))
            else:
                all.append('-1')

            q.put(all)
            # print("---------------------------------------------------------------")

    results = []
    if __name__ == "__main__":

        m = Manager()
        q = m.Queue()  # use this manager Queue instead of multiprocessing Queue as that causes error
        p = {}
        a = []
        url = []

        # Hacky fix
        words = product_name.split()
        var = len(words)

        if var == 1:
            url = "https://www.amazon.com/s?k=" + words[0]
            print(url)
        if var == 2:
            url = "https://www.amazon.com/s?k=" + words[0] + "+" + words[1]
            print(url)
        elif var == 3:
            url = "https://www.amazon.com/s?k=" + words[0] + "+" + words[1] + "+" + words[2]
            print(url)
        elif var == 4:
            url = "https://www.amazon.com/s?k=" + words[0] + "+" + words[1] + "+" + words[2] + "+" + words[3]
            print(url)
        elif var == 5:
            url = "https://www.amazon.com/s?k=" + words[0] + "+" + words[1] + "+" + words[2] + "+" + words[3] + "+" + words[4]
            print(url)
        elif var == 6:
            url = "https://www.amazon.com/s?k=" + words[0] + "+" + words[1] + "+" + words[2] + "+" + words[3] + "+" + words[4]+ "+" + words[5]
            print(url)
        elif var == 6:
            url = "https://www.amazon.com/s?k=" + words[0] + "+" + words[1] + "+" + words[2] + "+" + words[3] + "+" + words[4]+ "+" + words[5]+ "+" + words[6]
            print(url)

        for i in range(1, no_pages):
            print("starting thread: ", i)
            p[i] = threading.Thread(target=get_data, args=(i, q))        #function call get_data
            p[i].start()

        for i in range(1, no_pages):
            p[i].join()

        while q.empty() is not True:
            qcount = qcount + 1
            queue_top = q.get()
            products.append(queue_top[0])
            prices.append(queue_top[1])
            ratings.append(queue_top[2])
            links.append(queue_top[3])

        print("total time taken: ", str(time.time() - startTime), " qcount: ", qcount)

        df = pd.DataFrame({'Product_Name': products, 'Price': prices, 'Ratings': ratings, 'Product_Link': links})
        df.to_csv('./amazon/amazon_' + time_str + '.csv', index=False, encoding='utf-8')

        data = pd.read_csv('./amazon/amazon_' + time_str + '.csv')
        data['rating_new'] = data['Ratings'].astype(str).str[:3]
        data['rating_new'] = data['rating_new'].astype(float)
        data.loc[(data['rating_new'] == -1.0), 'rating_new'] = 0
        data['Price'] = data['Price'].str[1:]
        data['Price'] = data['Price'].apply(lambda x: float(x.split()[0].replace(',', '')))
        data['Price'] = data['Price'].astype(float)
        data = data[data[["Price"]].apply(np.isclose, b= price, atol= price2).any(1)]   #price
        data = data.sort_values(['rating_new', 'Price'], ascending=[False, False])
        print(data.head(5))
        return json.dumps({"Recommendations": (data[data.columns[0::3]].head(5).to_json()), "total time taken: ": str(time.time() - startTime)
                              ,"item_count": qcount, "code": 100})


if __name__ == '__main__':
    app.run(debug=True)